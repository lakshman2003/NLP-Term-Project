{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7OnPaVYwxJZ",
        "outputId": "489f609f-dae6-4dfe-8646-2bc03e983331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 6.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 73.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from collections import OrderedDict\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
        "#import nltk\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "#from nltk.tokenize import SpaceTokenizer\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ImRBcHnh7kw",
        "outputId": "d80f6c7d-d7e9-4ac0-8748-7a77b5271ef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def BERTData(sent_file, pointer_file, pos_file, ent_file, bert_sent_file, bert_pointer_file, bert_pos_file, bert_ent_file):#only changed here\n",
        "    bert_model_name = 'bert-base-cased'\n",
        "    sents = open(sent_file).readlines()\n",
        "    pos_lists = open(pos_file).readlines()\n",
        "    ent_lists = open(ent_file).readlines()#\n",
        "    pointer_lines = open(pointer_file).readlines()\n",
        "    #bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\", do_basic_tokenize=False)\n",
        "\n",
        "    writer1 = open(bert_sent_file, 'w+')\n",
        "    writer2 = open(bert_pointer_file, 'w+')\n",
        "    writer3 = open(bert_pos_file, 'w+')\n",
        "    writer4 = open(bert_ent_file, 'w+')\n",
        "    max_len = 0\n",
        "    max_len_tok = 0\n",
        "    max_trg_len=0\n",
        "    max_len_et=0\n",
        "    for i in tqdm(range(len(sents))):\n",
        "        sent = sents[i].strip()#sentence\n",
        "        pos_seq = pos_lists[i].strip().split()#pos tag list\n",
        "        ent_seq = ent_lists[i].strip().split()#ent tag\n",
        "        #print(pos_seq)\n",
        "        #pos_seq = list()\n",
        "        #tk = SpaceTokenizer()\n",
        "        #tags = nltk.pos_tag(tk.tokenize(sent))\n",
        "        #for t in tags:\n",
        "            #t1 = t[1]\n",
        "            #pos_seq.append(t1)#all the corresponding pos tags\n",
        "\n",
        "        tokens = sent.split(' ')#token list\n",
        "        bert_tokens = []###\n",
        "        token_map = OrderedDict()\n",
        "        bert_idx = 0\n",
        "        bert_pos_seq = []###\n",
        "        bert_ent_seq = []###\n",
        "        for j in range(len(tokens)):\n",
        "            sub_tokens = bert_tokenizer.tokenize(tokens[j])\n",
        "            if len(sub_tokens) == 0:\n",
        "                sub_tokens = [tokens[j]]\n",
        "            bert_tokens += sub_tokens\n",
        "            token_map[j] = (bert_idx, bert_idx + len(sub_tokens) - 1)\n",
        "            bert_idx += len(sub_tokens)\n",
        "            bert_pos_seq += [pos_seq[j] for k in range(len(sub_tokens))]\n",
        "            bert_ent_seq += [ent_seq[j] for l in range(len(sub_tokens))]\n",
        "\n",
        "        #print(bert_pos_seq)\n",
        "        assert len(bert_tokens) == len(bert_pos_seq)\n",
        "        assert len(bert_tokens) == len(bert_ent_seq)\n",
        "\n",
        "        if max_len < len(bert_pos_seq):\n",
        "            max_len = len(bert_pos_seq)\n",
        "\n",
        "        if max_len_tok < len(bert_tokens):\n",
        "            max_len_tok = len(bert_tokens)\n",
        "\n",
        "        if max_len_et < len(bert_ent_seq):\n",
        "            max_len_et = len(bert_ent_seq)\n",
        "\n",
        "        bert_pointers = []\n",
        "\n",
        "        pointer_line = pointer_lines[i].strip()\n",
        "        pointers = pointer_line.split(' | ')\n",
        "        for p in pointers:\n",
        "            if(len(p.split())==0):\n",
        "              continue\n",
        "            t_s, t_e, ev, a_s, a_e, ar, role = p.split()\n",
        "            new_p = [str(token_map[int(t_s)][0]), str(token_map[int(t_e)][1]), ev, str(token_map[int(a_s)][0]), str(token_map[int(a_e)][1]), ar, role]\n",
        "            bert_pointers.append(' '.join(new_p))\n",
        "        #print(bert_pointers)\n",
        "        if max_trg_len < len(bert_pointers):\n",
        "            max_trg_len = len(bert_pointers)\n",
        "        bert_sent = ' '.join(bert_tokens)\n",
        "        bert_pos = ' '.join(bert_pos_seq)\n",
        "        bert_ent = ' '.join(bert_ent_seq)\n",
        "        bert_pointer_line = ' | '.join(bert_pointers)\n",
        "        # print(sent)\n",
        "        # print(pointer_line)\n",
        "        # print(bert_sent)\n",
        "        # print(bert_pointer_line)\n",
        "        # print('\\n\\n')\n",
        "        writer1.write(bert_sent + '\\n')\n",
        "        writer2.write(bert_pointer_line + '\\n')\n",
        "        writer3.write(bert_pos + '\\n')\n",
        "        writer4.write(bert_ent + '\\n')\n",
        "    writer1.close()\n",
        "    writer2.close()\n",
        "    writer3.close()\n",
        "    writer4.close()\n",
        "    return max_len_tok, max_len, max_trg_len, max_len_et"
      ],
      "metadata": {
        "id": "C0kpiApPw4uO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_sent.txt'#print(sys.argv[1])\n",
        "point_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_pointer.txt'#print(sys.argv[2])\n",
        "pos_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_pos.txt'#print(sys.argv[3])\n",
        "ent_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_dep.txt'\n",
        "bert_sent_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/bert_data/test/test_bert_sent.txt'#print(sys.argv[4])\n",
        "bert_point_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/bert_data/test/test_bert_pointer.txt'#print(sys.argv[5])\n",
        "bert_pos_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/bert_data/test/test_bert_pos.txt'\n",
        "bert_ent_file='/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/bert_data/test/test_bert_dep.txt'\n",
        "#BERTData(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n",
        "max1, max2, max3, max4 = BERTData(sent_file, point_file, pos_file, ent_file, bert_sent_file, bert_point_file, bert_pos_file, bert_ent_file)\n",
        "print('{},{},{},{}'.format(max1, max2, max3, max4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmU4bUiLw8-M",
        "outputId": "9a6fc7ce-8b42-45e4-af89-f7075cb4bee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 205/205 [00:00<00:00, 212.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1071,1071,23,1071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eSJHAxTT4eTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}