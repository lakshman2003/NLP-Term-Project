{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Parsing  \n",
        "\n",
        "## This code takes the raw xml files as input and finds the sentence embedded in the file and also the seven element tuples which are the ouptut for the sentence."
      ],
      "metadata": {
        "id": "9R4nMqqD3zC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "LpCqGIijSFbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Pwoq0BQSfjmq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ae524e4-10bd-40de-b61d-764791866494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_sentence(xml_data):\n",
        "  root = ET.XML(xml_data)\n",
        "  children = root.getchildren()\n",
        "  #print(children)\n",
        "  raw_xml = []\n",
        "  for i,child in enumerate(children):\n",
        "   if child.tag=='P':\n",
        "     raw_xml.append(child.getchildren())\n",
        "  cnt = 0;\n",
        "  ent_type = set()\n",
        "  event_type = set()\n",
        "  sentence = []\n",
        "  for i, raw_sent in enumerate(raw_xml):\n",
        "      for j,word in enumerate(raw_sent):\n",
        "          if word.tag=='W':\n",
        "              text = word.text.strip()\n",
        "              if(text.find('http')!=-1 or text.find('url')!=-1):\n",
        "                continue\n",
        "              sentence.append(word.text.strip())\n",
        "              cnt+=1\n",
        "          else:\n",
        "              #print(word.tag)\n",
        "              tags = word.getchildren()\n",
        "              #print(tags)\n",
        "              for tag in tags:\n",
        "                  if tag.tag=='W':\n",
        "                    sentence.append(tag.text.strip())\n",
        "\n",
        "  sentence = \" \".join(sentence)\n",
        "  #print(sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "C6JiGZjfh0y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos(sentence):\n",
        "  doc = nlp(sentence)\n",
        "  pos_tags = []\n",
        "  for token in doc:\n",
        "    pos_tags.append(token.pos_)\n",
        "  pos_sent = \" \".join(pos_tags)\n",
        "  return pos_sent"
      ],
      "metadata": {
        "id": "dJLW--0Zn5hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dep(sentence):\n",
        "  nlp = spacy.load(\"en_core_web_sm\")\n",
        "  doc = nlp(sentence)\n",
        "  dep = []\n",
        "  for token in doc:\n",
        "    dep.append(token.dep_)\n",
        "  dep_sent = \" \".join(dep)\n",
        "  return dep_sent"
      ],
      "metadata": {
        "id": "zrZUgYRpoFm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Input to Output Tuples**"
      ],
      "metadata": {
        "id": "YslSuqsUYV04"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **FINAL FUNCTION**"
      ],
      "metadata": {
        "id": "drHd0eHaiN76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET"
      ],
      "metadata": {
        "id": "YX-GX1XEiRvW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def return_tuples(sentence):\n",
        "    final_tuples = []\n",
        "    event_dict = {}\n",
        "\n",
        "    for i in range(len(sentence)):\n",
        "        if sentence[i][0]=='#':\n",
        "            temp = sentence[i]\n",
        "            temp = temp.split()\n",
        "            key = temp[1]\n",
        "            event_dict[key] = 0\n",
        "\n",
        "    for i in range(len(sentence)):\n",
        "\n",
        "        if sentence[i-1][0]!='$' and sentence[i][0]=='$':\n",
        "            \n",
        "            temp = sentence[i]\n",
        "            temp = temp.split()\n",
        "            temp_id =  temp[1]\n",
        "            #print(temp_id)\n",
        "            event_dict[temp_id] += 1\n",
        "            \n",
        "            start_ind_event=0\n",
        "            end_ind_event=0\n",
        "            \n",
        "            count=0\n",
        "            event_word = ''\n",
        "            for j in range(len(sentence)):\n",
        "                temp0 = sentence[j]\n",
        "                temp0 = temp0.split()\n",
        "\n",
        "                if temp0[0]=='#' and temp0[1]==temp_id:\n",
        "                    count+=1\n",
        "                    if count<=1:\n",
        "                        start_ind_event = j\n",
        "                        event_word = temp0[2]\n",
        "                    end_ind_event = j\n",
        "\n",
        "            start_ind_argument=i\n",
        "            end_ind_argumnent=i\n",
        "\n",
        "            k=i\n",
        "            while(k<len(sentence)):\n",
        "                if sentence[k][0]=='$':\n",
        "                    end_ind_argumnent=k\n",
        "                    k=k+1\n",
        "                else:\n",
        "                    break    \n",
        "\n",
        "            x = sentence[i].split()\n",
        "            argument_word = x[2]\n",
        "\n",
        "            output_tuple = [str(start_ind_event), str(end_ind_event), event_word, str(start_ind_argument), str(end_ind_argumnent), 'ARG',argument_word]\n",
        "            final_tuples.append(output_tuple)\n",
        "            #print(output_tuple)\n",
        "    \n",
        "    if len(event_dict)==0:\n",
        "        output_tuple = [str(0), str(0), 'NA', str(1), str(1), 'NA','NA']\n",
        "    \n",
        "    for key in event_dict:\n",
        "        if event_dict[key]:\n",
        "            start=0\n",
        "            end=0\n",
        "            count=0\n",
        "            event_word = ''\n",
        "            for j in range(len(sentence)):\n",
        "                if sentence[j][0]=='#' and sentence[j][2]==temp_id:\n",
        "                    count+=1\n",
        "                    if count<=1:\n",
        "                        start = j\n",
        "                        x = sentence[j].split()\n",
        "                        event_word = x[2]\n",
        "                    end = j            \n",
        "            output_tuple = [str(start), str(end), event_word, str(1), str(1), 'NA','NA']             \n",
        "\n",
        "    return final_tuples"
      ],
      "metadata": {
        "id": "AwnsGYjxiWpG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_tuple_to_string(final_tuples):\n",
        "    final_string = ''\n",
        "    for _tup_ in final_tuples:\n",
        "        for token in _tup_:\n",
        "            final_string+=token\n",
        "            final_string+=' '\n",
        "        final_string += '| '\n",
        "    final_string = final_string[0:len(final_string)-3]\n",
        "    return final_string"
      ],
      "metadata": {
        "id": "JM8S_pariemc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roles = []"
      ],
      "metadata": {
        "id": "t6HB4FKjJ_9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_string(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    elements = []\n",
        "    numbers = []\n",
        "    sentences = []\n",
        "    tags = []\n",
        "    sentences.append('UNK')\n",
        "    sentences.append('UNK')\n",
        "\n",
        "    for child in root:\n",
        "        if child.tag=='P':\n",
        "            element = []\n",
        "            number = []\n",
        "            \n",
        "            if child.tag not in tags:\n",
        "                tags.append(child.tag)\n",
        "\n",
        "            for x in child:    \n",
        "                if x.tag=='W':\n",
        "                  word = x.text.strip()\n",
        "                  if (word.find(\"http\")!=-1 or word.find('url')!=-1):\n",
        "                    continue\n",
        "\n",
        "                if x.tag not in tags:\n",
        "                    tags.append(x.tag)\n",
        "\n",
        "\n",
        "                element.append(x)\n",
        "                number.append(1)\n",
        "                if x.tag=='W':\n",
        "                    sentences.append(x.text.strip())\n",
        "                for y in x:\n",
        "                    \n",
        "                    if y.tag=='LINK':\n",
        "                        last_id = y.attrib[\"EVENT_ARG\"]\n",
        "\n",
        "                    if y.tag=='W' and x.tag[len(x.tag)-5:len(x.tag)]=='EVENT':\n",
        "                        sentences.append('# '+x.attrib[\"ID\"]+\" \"+x.tag+\":\"+x.attrib[\"TYPE\"]+\" \"+y.text.strip())\n",
        "                    if y.tag=='W' and x.tag[len(x.tag)-3:len(x.tag)]=='ARG':\n",
        "                        sentences.append('$ '+str(last_id)+\" \"+x.tag[0:len(x.tag)-4]+\" \"+y.text.strip())\n",
        "                        role = x.tag[0:len(x.tag)-4]\n",
        "                        if role not in roles:\n",
        "                          roles.append(role)\n",
        "    #print(sentences)\n",
        "    #print(return_tuples(sentences))\n",
        "    return convert_tuple_to_string(return_tuples(sentences))  "
      ],
      "metadata": {
        "id": "nFKH1J1Aihdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events = []"
      ],
      "metadata": {
        "id": "kfhbutztEWmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_events(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    for child in root:\n",
        "        for x in child:\n",
        "            if x.tag[len(x.tag)-5:len(x.tag)]=='EVENT':\n",
        "                string = x.tag+\":\"+x.attrib[\"TYPE\"]\n",
        "                if string not in events:\n",
        "                    events.append(string)\n",
        "    return None"
      ],
      "metadata": {
        "id": "AAe-bG0uuiFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_string('file185.xml')"
      ],
      "metadata": {
        "id": "WvI_7njZijxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using the above functions on train,dev and test sets"
      ],
      "metadata": {
        "id": "cq-y37HO0W8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# sentence_file = open('test_sent.txt',\"a\")\n",
        "# pointer_file = open('test_pointer.txt',\"a\")\n",
        "# dep_file = open('test_dep.txt',\"a\")\n",
        "# pos_file = open('test_pos.txt',\"a\")\n",
        "\n",
        "sentences = []\n",
        "pos_tags= []\n",
        "dep_tags = []\n",
        "pointers = []\n",
        "error_files = []\n",
        "correct_files = []\n",
        "folder_path = '/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/test'\n",
        "os.chdir(folder_path)\n",
        "for file in os.listdir():\n",
        "  #print(file)\n",
        "  file_path = f\"{folder_path}/{file}\"\n",
        "  xml_data = open(file_path, 'r').read()\n",
        "  try:\n",
        "    sentence = read_sentence(xml_data)\n",
        "  except:\n",
        "    error_files.append(file)\n",
        "    continue\n",
        "  pos_sent = get_pos(sentence)\n",
        "  dep_sent = get_dep(sentence)\n",
        "  sentence = \"[unused1] [unused2] \"+ sentence\n",
        "  #pos_sent = get_pos(sentence)\n",
        "  pos_sent = \"<UNK> <UNK> \"+pos_sent\n",
        "  #dep_sent = get_dep(sentence)\n",
        "  dep_sent = \"<UNK> <UNK> \"+dep_sent\n",
        "  #pointer_sent = get_string(file_path)\n",
        "  try:\n",
        "    pointer_sent = get_string(file_path)\n",
        "  except:\n",
        "    error_files.append(file)\n",
        "    continue\n",
        "  add_events(file_path)\n",
        "  #print(sentence)\n",
        "  # sentence_file.write(sentence +'\\n')\n",
        "  # pos_file.write(pos_sent+'\\n')\n",
        "  # dep_file.write(dep_sent+'\\n')\n",
        "  # pointer_file.write(pointer_sent+'\\n')\n",
        "  correct_files.append(file)\n",
        "  sentences.append(sentence)\n",
        "  pos_tags.append(pos_sent)\n",
        "  dep_tags.append(dep_sent)\n",
        "  pointers.append(pointer_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oogSisjAntJs",
        "outputId": "2058f8f5-2813-454c-d9ea-27ac074c6fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: This method will be removed in future versions.  Use 'list(elem)' or iteration over elem instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#file = '/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/test/file400.xml'\n",
        "# file = open(file,'r').read()\n",
        "# print(read_sentence(file))\n",
        "\n",
        "#print(get_string(file))"
      ],
      "metadata": {
        "id": "oCnHpSxaFGbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_sent.txt','w') as file:\n",
        "  pass\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_sent.txt','a') as file:\n",
        "  for sent in sentences:\n",
        "    file.write(sent+'\\n')"
      ],
      "metadata": {
        "id": "04PuaA47Fazo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_pointer.txt','w') as file:\n",
        "  pass\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_pointer.txt','a') as file:\n",
        "  for sent in pointers:\n",
        "    file.write(sent+'\\n')"
      ],
      "metadata": {
        "id": "UxGTjX3uSrJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_pos.txt','w') as file:\n",
        "  pass\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_pos.txt','a') as file:\n",
        "  for sent in pos_tags:\n",
        "    file.write(sent+'\\n')"
      ],
      "metadata": {
        "id": "NX3ai3qGS7lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_dep.txt','w') as file:\n",
        "  pass\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_dep.txt','a') as file:\n",
        "  for sent in dep_tags:\n",
        "    file.write(sent+'\\n')"
      ],
      "metadata": {
        "id": "GD73lQF8TDKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/tested_files.txt','w') as file:\n",
        "  pass\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/tested_files.txt','a') as file:\n",
        "  for x in correct_files:\n",
        "    file.write(x)\n",
        "    file.write('\\n')"
      ],
      "metadata": {
        "id": "84EIb5Y5EHeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_error_files.txt','w') as file:\n",
        "  pass\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/test/test_error_files.txt','a') as file:\n",
        "  for x in error_files:\n",
        "    file.write(x)\n",
        "    file.write('\\n')"
      ],
      "metadata": {
        "id": "_kNJkMUcFIJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/ent_type.txt','a') as file:\n",
        "#   file.write(\"ARG\\n\")\n",
        "#   file.write(\"NA\")"
      ],
      "metadata": {
        "id": "g4EdYnZGFHhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/event_type.txt','r') as file:\n",
        "  curr_events = file.readlines()\n",
        "  curr_events = [x.strip() for x in curr_events]\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/event_type.txt','a') as file:\n",
        "  for event in events:\n",
        "    if event not in curr_events:\n",
        "      file.write(event+'\\n')"
      ],
      "metadata": {
        "id": "7xhwEZTyFoup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#curr_events = [x.strip() for x in curr_events]"
      ],
      "metadata": {
        "id": "4gxyR7L-SKf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#curr_events"
      ],
      "metadata": {
        "id": "_nbwMr-NSTYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/role.txt','r') as file:\n",
        "  curr_roles = file.readlines()\n",
        "  curr_roles = [x.strip() for x in curr_roles]\n",
        "with open('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/raw_data/role.txt','a') as file:\n",
        "  for role in roles:\n",
        "    if role not in curr_roles:\n",
        "      file.write(role+'\\n')\n",
        "  # file.write(\"NA\")"
      ],
      "metadata": {
        "id": "1V9vmwHuKhRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentences[177]"
      ],
      "metadata": {
        "id": "1na0xJqIMEK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentences[550]"
      ],
      "metadata": {
        "id": "ldK2_qIzMZRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#correct_files[550]"
      ],
      "metadata": {
        "id": "fiLeqL7rQLFj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get_string('/content/drive/My Drive/NLP_Term_Project/dataset/english_ee/test/file977.xml')"
      ],
      "metadata": {
        "id": "Z7Ay9HkjRjv9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roles"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Seql8R8BR4d8",
        "outputId": "a9922167-4e48-426c-fb29-d4b0ad5aeadc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['CASUALTIES',\n",
              " 'PLACE',\n",
              " 'PARTICIPANT',\n",
              " 'TIME',\n",
              " 'REASON',\n",
              " 'AFTER_EFFECTS',\n",
              " 'INTENSITY',\n",
              " 'TEMPERATURE',\n",
              " 'MAGNITUDE',\n",
              " 'DEPTH',\n",
              " 'EPICENTRE',\n",
              " 'NAME',\n",
              " 'SPEED',\n",
              " 'TYPE']"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY8t_J7VQxzn",
        "outputId": "c093b90c-bf46-4c20-df19-52c15b145482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['MAN_MADE_EVENT:SHOOT_OUT',\n",
              " 'NATURAL_EVENT:FOREST_FIRE',\n",
              " 'NATURAL_EVENT:AVALANCHES',\n",
              " 'MAN_MADE_EVENT:NORMAL_BOMBING',\n",
              " 'MAN_MADE_EVENT:SUICIDE_ATTACK',\n",
              " 'NATURAL_EVENT:HEAT_WAVE',\n",
              " 'MAN_MADE_EVENT:TRAIN_COLLISION',\n",
              " 'MAN_MADE_EVENT:AVIATION_HAZARD',\n",
              " 'MAN_MADE_EVENT:SURGICAL_STRIKES',\n",
              " 'MAN_MADE_EVENT:TRANSPORT_HAZARDS',\n",
              " 'MAN_MADE_EVENT:INDUSTRIAL_ACCIDENT',\n",
              " 'MAN_MADE_EVENT:FIRE',\n",
              " 'NATURAL_EVENT:FLOODS',\n",
              " 'NATURAL_EVENT:LAND_SLIDE',\n",
              " 'MAN_MADE_EVENT:VEHICULAR_COLLISION',\n",
              " 'NATURAL_EVENT:STORM',\n",
              " 'NATURAL_EVENT:HURRICANE',\n",
              " 'NATURAL_EVENT:CYCLONE',\n",
              " 'NATURAL_EVENT:EARTHQUAKE',\n",
              " 'NATURAL_EVENT:HAIL_STORMS',\n",
              " 'NATURAL_EVENT:TORNADO',\n",
              " 'NATURAL_EVENT:VOLCANO',\n",
              " 'MAN_MADE_EVENT:TERRORIST_ATTACK',\n",
              " 'MAN_MADE_EVENT:ARMED_CONFLICTS',\n",
              " 'MAN_MADE_EVENT:MISCELLANEOUS',\n",
              " 'MAN_MADE_EVENT:CRIME',\n",
              " 'MAN_MADE_EVENT:ACCIDENTS',\n",
              " 'NATURAL_EVENT:DROUGHT',\n",
              " 'MAN_MADE_EVENT:RIOTS']"
            ]
          },
          "metadata": {},
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UF7xFRBzQ_cn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}